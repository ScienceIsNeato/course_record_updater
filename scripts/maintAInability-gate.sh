#!/bin/bash

# maintAInability-gate - Course Record Updater Quality Framework
# This script ensures code maintainability through comprehensive quality checks
# Mirrors Git Hooks & CI exactly - if this passes, your commit WILL succeed
#
# A Python/Flask quality gate that validates:
# - Code formatting and consistency (with auto-fix using black, isort)
# - Python linting and best practices (with auto-fix using flake8, pylint)
# - Type safety and analysis (mypy strict mode)
# - Test coverage and reliability (pytest with 80% coverage gate)
# - Security vulnerability scanning (bandit, safety)
# - Code duplication prevention
# - Import organization and analysis
#
# Usage:
#   ./scripts/maintAInability-gate.sh           # All checks (strict mode with auto-fix)
#   ./scripts/maintAInability-gate.sh --format  # Check/fix formatting only
#   ./scripts/maintAInability-gate.sh --lint    # Check/fix linting only
#   ./scripts/maintAInability-gate.sh --types   # Check types only
#   ./scripts/maintAInability-gate.sh --tests   # Run tests with 80% coverage gate
#   ./scripts/maintAInability-gate.sh --security # Check security vulnerabilities
#   ./scripts/maintAInability-gate.sh --duplication # Check code duplication
#   ./scripts/maintAInability-gate.sh --imports # Check import organization
#   ./scripts/maintAInability-gate.sh --help    # Show this help

set -e

# Check required environment variables (skip in CI environments)
if [ "${CI:-false}" = "true" ] || [ "${GITHUB_ACTIONS:-false}" = "true" ]; then
  echo "ğŸ”„ Skipping environment variable check in CI environment"
else
  echo "ğŸ” Checking environment variables..."

  REQUIRED_VARS=(
    "AGENT_HOME"
    "DATABASE_TYPE"
    "DATABASE_URL"
    "COURSE_RECORD_UPDATER_PORT"
    "SONAR_TOKEN"
    "SAFETY_API_KEY"
    "DEFAULT_PORT"
    "GITHUB_PERSONAL_ACCESS_TOKEN"
  )

  MISSING_VARS=()

  for var in "${REQUIRED_VARS[@]}"; do
    if [ -z "${!var}" ]; then
      MISSING_VARS+=("$var")
    fi
  done

  if [ ${#MISSING_VARS[@]} -ne 0 ]; then
    echo "âŒ ENVIRONMENT SETUP FAILED"
    echo ""
    echo "Missing required environment variables:"
    for var in "${MISSING_VARS[@]}"; do
      echo "  â€¢ $var"
    done
    echo ""
    echo "ğŸ’¡ FIX: Run 'direnv allow' to load environment variables from .envrc"
    echo "   This is a common issue - the .envrc file contains all required variables"
    echo "   but direnv needs permission to load them into your shell environment."
    echo ""
    echo "   If direnv is not installed: brew install direnv"
    echo "   Then add to your shell config: eval \"\$(direnv hook bash)\" or eval \"\$(direnv hook zsh)\""
    exit 1
  fi

  echo "âœ… Environment variables loaded correctly"
fi

# Individual check flags - ATOMIC CHECKS ONLY
RUN_BLACK=false
RUN_ISORT=false
RUN_LINT=false
RUN_TYPES=false
RUN_TESTS=false
RUN_INTEGRATION_TESTS=false
RUN_COVERAGE=false
RUN_SECURITY=false
RUN_SONAR=false
RUN_DUPLICATION=false
RUN_IMPORTS=false
RUN_COMPLEXITY=false
RUN_JS_LINT=false
RUN_JS_FORMAT=false
RUN_JS_TESTS=false
RUN_JS_COVERAGE=false
RUN_ALL=false

# Parse arguments
if [ $# -eq 0 ]; then
  RUN_ALL=true
else
  while [[ $# -gt 0 ]]; do
    case $1 in
      --black) RUN_BLACK=true ;;
      --isort) RUN_ISORT=true ;;
      --lint) RUN_LINT=true ;;
      --types) RUN_TYPES=true ;;
      --tests) RUN_TESTS=true ;;
      --integration-tests) RUN_INTEGRATION_TESTS=true ;;
      --coverage) RUN_COVERAGE=true ;;
      --security) RUN_SECURITY=true ;;
      --sonar) RUN_SONAR=true ;;
      --duplication) RUN_DUPLICATION=true ;;
      --imports) RUN_IMPORTS=true ;;
      --complexity) RUN_COMPLEXITY=true ;;
      --js-lint) RUN_JS_LINT=true ;;
      --js-format) RUN_JS_FORMAT=true ;;
      --js-tests) RUN_JS_TESTS=true ;;
      --js-coverage) RUN_JS_COVERAGE=true ;;
      --smoke-tests) RUN_SMOKE_TESTS=true ;;
      --frontend-check) RUN_FRONTEND_CHECK=true ;;
      --help)
        echo "maintAInability-gate - Course Record Updater Quality Framework"
        echo ""
        echo "Usage:"
        echo "  ./scripts/maintAInability-gate.sh           # All atomic checks (strict mode with auto-fix)"
        echo "  ./scripts/maintAInability-gate.sh --black   # Check/fix code formatting only"
        echo "  ./scripts/maintAInability-gate.sh --isort   # Check/fix import sorting only"
        echo "  ./scripts/maintAInability-gate.sh --lint    # Check/fix linting only"
        echo "  ./scripts/maintAInability-gate.sh --types   # Check types only"
        echo "  ./scripts/maintAInability-gate.sh --tests   # Run test suite only"
        echo "  ./scripts/maintAInability-gate.sh --coverage # Run coverage analysis only"
        echo "  ./scripts/maintAInability-gate.sh --security # Check security vulnerabilities"
        echo "  ./scripts/maintAInability-gate.sh --sonar   # Run SonarQube quality analysis"
        echo "  ./scripts/maintAInability-gate.sh --js-tests # Run JavaScript test suite (Jest)"
        echo "  ./scripts/maintAInability-gate.sh --js-coverage # Run JavaScript coverage analysis"
        echo "  ./scripts/maintAInability-gate.sh --duplication # Check code duplication"
        echo "  ./scripts/maintAInability-gate.sh --imports # Check import organization"
        echo "  ./scripts/maintAInability-gate.sh --complexity # Check code complexity"
        echo "  ./scripts/maintAInability-gate.sh --js-lint    # Check JavaScript linting"
        echo "  ./scripts/maintAInability-gate.sh --js-format  # Check JavaScript formatting"
        echo "  ./scripts/maintAInability-gate.sh --smoke-tests # Run smoke tests (tests/smoke/)"
        echo "  ./scripts/maintAInability-gate.sh --frontend-check # Quick frontend validation"
        echo "  ./scripts/maintAInability-gate.sh --help    # Show this help"
        echo ""
        echo "This script ensures code maintainability through comprehensive quality checks"
        echo "and mirrors Git Hooks & CI exactly - if this passes, your commit WILL succeed."
        exit 0
        ;;
      *) echo "Unknown option: $1"; echo "Use --help for usage information"; exit 1 ;;
    esac
    shift
  done
fi

# Set all flags if RUN_ALL is true - ATOMIC CHECKS ONLY
if [[ "$RUN_ALL" == "true" ]]; then
  RUN_BLACK=true
  RUN_ISORT=true
  RUN_LINT=true
  RUN_TESTS=true
  RUN_COVERAGE=true
  RUN_TYPES=true
  RUN_SECURITY=true
  RUN_SONAR=true  # Enabled - SonarCloud project is configured
  RUN_DUPLICATION=true
  RUN_IMPORTS=true
  RUN_JS_LINT=true
  RUN_JS_FORMAT=true
  RUN_JS_TESTS=true
  RUN_JS_COVERAGE=true
fi

# Track failures with detailed information
FAILED_CHECKS=0
FAILED_CHECKS_DETAILS=()
PASSED_CHECKS=()

# Helper function to add failure details
add_failure() {
  local check_name="$1"
  local failure_reason="$2"
  local fix_suggestion="$3"

  ((FAILED_CHECKS++))
  FAILED_CHECKS_DETAILS+=("$check_name|$failure_reason|$fix_suggestion")
}

# Helper function to add passed check
add_success() {
  local check_name="$1"
  local success_message="$2"

  PASSED_CHECKS+=("$check_name|$success_message")
}

# Check virtual environment (skip in CI)
check_venv() {
  # Skip venv check if we're in CI environment
  if [[ "$CI" == "true" ]]; then
    echo "â„¹ï¸  Skipping virtual environment check (CI environment detected)"
    return 0
  fi
  
  if [[ -z "$VIRTUAL_ENV" ]]; then
    echo "âš ï¸  Warning: Not in a virtual environment. Attempting to activate..."
    if [[ -f "venv/bin/activate" ]]; then
      source venv/bin/activate
      echo "âœ… Activated virtual environment"
    else
      echo "âŒ No virtual environment found. Please create one with: python -m venv venv && source venv/bin/activate"
      exit 1
    fi
  fi
}

echo "ğŸ” Running Course Record Updater quality checks (STRICT MODE with auto-fix)..."
echo "ğŸ Python/Flask enterprise validation suite"
echo ""

# Check virtual environment
check_venv


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# PYTHON BLACK FORMATTING CHECK (ATOMIC)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_BLACK" == "true" ]]; then
  echo "ğŸ¨ Code Formatting (black)"

  # Try to auto-fix formatting issues with black
  echo "ğŸ”§ Auto-fixing code formatting with black..."
  if black --line-length 88 --target-version py39 *.py adapters/ tests/ 2>/dev/null || true; then
    echo "   âœ… Black formatting applied"
  fi

  # Verify formatting
  if black --check --line-length 88 --target-version py39 *.py adapters/ tests/ > /dev/null 2>&1; then
    echo "âœ… Black Check: PASSED (code formatting verified)"
    add_success "Black Check" "All Python files properly formatted with black"
  else
    echo "âŒ Black Check: FAILED (formatting issues found)"
    add_failure "Black Check" "Code formatting issues found" "Run 'black *.py adapters/ tests/' manually"
  fi
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# PYTHON ISORT IMPORT SORTING CHECK (ATOMIC)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_ISORT" == "true" ]]; then
  echo "ğŸ“š Import Sorting (isort)"

  # Try to auto-fix import organization with isort
  echo "ğŸ”§ Auto-fixing import organization with isort..."
  if isort --profile black *.py adapters/ tests/ 2>/dev/null || true; then
    echo "   âœ… Import organization applied"
  fi

  # Verify import organization
  if isort --check-only --profile black *.py adapters/ tests/ > /dev/null 2>&1; then
    echo "âœ… Isort Check: PASSED (import organization verified)"
    add_success "Isort Check" "All Python imports properly organized with isort"
  else
    echo "âŒ Isort Check: FAILED (import organization issues found)"
    add_failure "Isort Check" "Import organization issues found" "Run 'isort --profile black *.py adapters/ tests/' manually"
  fi
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# PYTHON LINT CHECK (FLAKE8 + BASIC PYLINT)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_LINT" == "true" ]]; then
  echo "ğŸ” Python Lint Check (flake8 critical errors)"

  # Run flake8 for critical errors only (much faster)
  echo "ğŸ”§ Running flake8 critical error check..."
  FLAKE8_OUTPUT=$(timeout 30s flake8 --max-line-length=88 --select=E9,F63,F7,F82 --exclude=venv,cursor-rules,.venv,logs,build-output *.py adapters/ tests/ 2>/dev/null) || FLAKE8_FAILED=true

  if [[ "$FLAKE8_FAILED" == "true" ]]; then
    echo "âŒ Flake8 critical errors found"
    echo "ğŸ“‹ Critical Issues:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "$FLAKE8_OUTPUT" | head -10 | sed 's/^/  /'
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    add_failure "Lint Check" "Critical linting errors found" "Fix the critical errors above"
  else
    echo "âœ… Lint Check: PASSED (no critical errors)"
    add_success "Lint Check" "No critical linting errors found"
  fi
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# TYPE CHECK (MYPY STRICT MODE)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_TYPES" == "true" ]]; then
  echo "ğŸ”§ Type Check (mypy strict mode)"

  # Run mypy type checking (main files only, with timeout)
  # Include scripts for type checking but exclude from coverage
  TYPE_OUTPUT=$(timeout 30s find . -name "*.py" -not -path "./venv/*" -not -path "./cursor-rules/*" -not -path "./.venv/*" -not -path "./tests/*" -not -path "./scripts/seed_db.py" | xargs mypy --ignore-missing-imports --no-strict-optional 2>&1) || TYPE_FAILED=true

  if [[ "$TYPE_FAILED" != "true" ]]; then
    echo "âœ… Type Check: PASSED (strict mypy type checking)"
    add_success "Type Check" "All type annotations valid in strict mode"
  else
    echo "âŒ Type Check: FAILED (strict mypy type checking)"
    echo ""

    # Show detailed type errors
    echo "ğŸ“‹ Type Error Details:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

    if [[ -n "$TYPE_OUTPUT" ]]; then
      echo "$TYPE_OUTPUT" | head -20 | sed 's/^/  /'

      # Check if there are more errors
      TOTAL_LINES=$(echo "$TYPE_OUTPUT" | wc -l)
      if [[ $TOTAL_LINES -gt 20 ]]; then
        echo "  ... and $(($TOTAL_LINES - 20)) more lines"
        echo "  Run 'mypy --strict *.py **/*.py' to see all errors"
      fi
    else
      echo "  Unable to extract type error details"
    fi

    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""

    # Extract error count from output for summary
    ERROR_COUNT=$(echo "$TYPE_OUTPUT" | grep -c "error:" || echo "unknown")
    add_failure "Type Check" "$ERROR_COUNT type errors found" "See detailed output above and run 'mypy --strict *.py **/*.py' for full details"
  fi
  echo ""
fi


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# TEST SUITE EXECUTION (ATOMIC) - UNIT TESTS ONLY
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_TESTS" == "true" ]]; then
  echo "ğŸ§ª Test Suite Execution (pytest)"

  # Run UNIT tests only (fast tests, separate directory, no coverage)
  echo "  ğŸ” Running UNIT test suite (tests only, no coverage)..."
  TEST_OUTPUT=$(python -m pytest tests/unit/ -v 2>&1) || TEST_FAILED=true

  if [[ "$TEST_FAILED" == "true" ]]; then
    echo "âŒ Tests: FAILED"
    echo ""

    # Show detailed test output with failures
    echo "ğŸ“‹ Test Failure Details:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

    # Extract and show failing tests
    FAILING_TESTS=$(echo "$TEST_OUTPUT" | grep "FAILED " | head -10)
    if [[ -n "$FAILING_TESTS" ]]; then
      echo "ğŸ”´ Failing Tests:"
      echo "$FAILING_TESTS" | sed 's/^/  /'
      echo ""
    fi

    # Show error summary - try multiple approaches to extract useful failure info
    SUMMARY_SECTION=$(echo "$TEST_OUTPUT" | grep -A 20 "short test summary info" | head -20)
    if [[ -n "$SUMMARY_SECTION" ]]; then
      echo "$SUMMARY_SECTION" | sed 's/^/  /'
    else
      # If no short summary, show FAILED test lines and any assertion errors
      FAILED_TESTS=$(echo "$TEST_OUTPUT" | grep -E "(FAILED|ERROR|AssertionError|assert)" | head -10)
      if [[ -n "$FAILED_TESTS" ]]; then
        echo "  Test failure details:"
        echo "$FAILED_TESTS" | sed 's/^/    /'
      else
        # Last resort: show last 10 lines of output which usually contain the error
        echo "  Last lines of test output:"
        echo "$TEST_OUTPUT" | tail -10 | sed 's/^/    /'
      fi
    fi

    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""

    # Extract summary stats for the failure record
    FAILED_TESTS=$(echo "$TEST_OUTPUT" | grep -o '[0-9]\+ failed' | head -1 || echo "unknown")
    add_failure "Test Execution" "Test failures: $FAILED_TESTS" "See detailed output above and run 'python -m pytest -v' for full details"
  else
    echo "âœ… Test Execution: PASSED"
    add_success "Test Execution" "All unit tests passed successfully"
  fi
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# INTEGRATION TEST SUITE EXECUTION - COMPONENT INTERACTIONS
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_INTEGRATION_TESTS" == "true" ]]; then
  echo "ğŸ”— Integration Test Suite Execution (tests/integration/)"
  
  echo "  ğŸ” Running INTEGRATION test suite (component interactions)..."
  INTEGRATION_TEST_OUTPUT=$(python -m pytest tests/integration/ -v 2>&1) || INTEGRATION_TEST_FAILED=true
  
  if [[ "$INTEGRATION_TEST_FAILED" == "true" ]]; then
    echo "âŒ Integration Tests: FAILED"
    echo ""
    echo "ğŸ“‹ Integration Test Failure Details:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "$INTEGRATION_TEST_OUTPUT" | sed 's/^/  /'
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    
    # Extract summary stats for the failure record
    FAILED_INTEGRATION_TESTS=$(echo "$INTEGRATION_TEST_OUTPUT" | grep -o '[0-9]\+ failed' | head -1 || echo "unknown")
    add_failure "Integration Tests" "Integration test failures: $FAILED_INTEGRATION_TESTS" "See detailed output above and run 'python -m pytest tests/integration/ -v' for full details"
  else
    echo "âœ… Integration Tests: PASSED"
    add_success "Integration Tests" "All integration tests passed successfully"
  fi
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# TEST COVERAGE ANALYSIS (ATOMIC) - 80% THRESHOLD
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_COVERAGE" == "true" ]]; then
  echo "ğŸ“Š Test Coverage Analysis (80% threshold)"

  # Run coverage analysis independently of test results (unit tests only)
  echo "  ğŸ“Š Running coverage analysis (independent of test results)..."
  
  # Ensure logs directory exists
  mkdir -p logs
  
  # Coverage report file (overwrite previous)
  COVERAGE_REPORT_FILE="logs/coverage_report.txt"
  
  # Run pytest with coverage but ignore test failures (--continue-on-collection-errors allows partial coverage)
  COVERAGE_OUTPUT=$(python -m pytest tests/unit/ --cov=. --cov-report=term-missing --tb=no --quiet 2>&1) || true
  
  # Write detailed coverage report to file
  echo "$COVERAGE_OUTPUT" > "$COVERAGE_REPORT_FILE"
  
  # Extract coverage percentage from output
  COVERAGE=$(echo "$COVERAGE_OUTPUT" | grep -o 'TOTAL.*[0-9]\+\.[0-9]\+%' | grep -o '[0-9]\+\.[0-9]\+%' | head -1 || echo "unknown")
  
  # Check if we got a valid coverage percentage
  if [[ "$COVERAGE" != "unknown" && "$COVERAGE" != "" ]]; then
    # Extract numeric value for comparison
    COVERAGE_NUM=$(echo "$COVERAGE" | sed 's/%//')
    
    # Coverage threshold with environment differences buffer
    # Base threshold: 80%
    # Environment buffer: 0.5% (accounts for differences between local macOS and CI Linux)
    # - Database connection paths may differ depending on DATABASE_URL
    # - Import conflict resolution logic may exercise different code paths
    # - Logging behavior can vary between environments
    THRESHOLD=80
    
    # Apply environment buffer only in CI (not locally)
    if [[ "${CI:-false}" == "true" ]]; then
      ENV_DIFFERENCES_BUFFER=0.5
      EFFECTIVE_THRESHOLD=$(echo "$THRESHOLD - $ENV_DIFFERENCES_BUFFER" | bc -l)
      echo "  ğŸ”§ CI environment detected - applying ${ENV_DIFFERENCES_BUFFER}% buffer (effective threshold: ${EFFECTIVE_THRESHOLD}%)"
    else
      ENV_DIFFERENCES_BUFFER=0
      EFFECTIVE_THRESHOLD=$THRESHOLD
      echo "  ğŸ  Local environment - using full ${THRESHOLD}% threshold"
    fi
    
    # Compare against effective threshold using bc for floating point
    if (( $(echo "$COVERAGE_NUM >= $EFFECTIVE_THRESHOLD" | bc -l) )); then
      echo "âœ… Coverage: PASSED ($COVERAGE)"
      add_success "Test Coverage" "Coverage at $COVERAGE (meets ${EFFECTIVE_THRESHOLD}% threshold with ${ENV_DIFFERENCES_BUFFER}% environment buffer)"
    else
      echo "âŒ Coverage: THRESHOLD NOT MET ($COVERAGE)"
      echo ""

      # Show coverage details
      echo "ğŸ“‹ Coverage Report:"
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      
      # Extract lines with missing coverage
      MISSING_LINES=$(echo "$COVERAGE_OUTPUT" | grep -E "TOTAL|Missing" | head -10)
      if [[ -n "$MISSING_LINES" ]]; then
        echo "$MISSING_LINES" | sed 's/^/  /'
      else
        echo "  Unable to extract coverage details"
      fi
      
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      
      # Show commit-specific coverage issues
      echo "ğŸ“‹ Files in Current Commit Needing Coverage:"
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      
      # Get list of Python files in the current commit (staged and unstaged changes)
      COMMIT_FILES=$(git diff --name-only HEAD 2>/dev/null | grep '\.py$' || echo "")
      if [[ -z "$COMMIT_FILES" ]]; then
        # If no diff with HEAD, try staged files
        COMMIT_FILES=$(git diff --cached --name-only 2>/dev/null | grep '\.py$' || echo "")
      fi
      if [[ -z "$COMMIT_FILES" ]]; then
        # If still no files, try unstaged changes
        COMMIT_FILES=$(git diff --name-only 2>/dev/null | grep '\.py$' || echo "")
      fi
      
      if [[ -n "$COMMIT_FILES" ]]; then
        echo "  Files in commit: $(echo "$COMMIT_FILES" | tr '\n' ' ')"
        echo ""
        
        # Filter coverage output to show only files in the commit
        COMMIT_COVERAGE=$(echo "$COVERAGE_OUTPUT" | grep -E "$(echo "$COMMIT_FILES" | sed 's/\.py$//' | tr '\n' '|' | sed 's/|$//')")
        if [[ -n "$COMMIT_COVERAGE" ]]; then
          echo "  Coverage details for commit files:"
          echo "$COMMIT_COVERAGE" | sed 's/^/    /'
        else
          echo "  No coverage issues found in commit files (issues may be in other files)"
        fi
      else
        echo "  No Python files found in current commit"
      fi
      
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      echo ""

      add_failure "Test Coverage" "Coverage at $COVERAGE (below ${EFFECTIVE_THRESHOLD}% threshold)" "Add tests to increase coverage above ${EFFECTIVE_THRESHOLD}%. Detailed report: $PWD/$COVERAGE_REPORT_FILE"
    fi
  else
    echo "âŒ Coverage: ANALYSIS FAILED"
    echo "ğŸ“‹ Coverage Output (for debugging):"
    echo "$COVERAGE_OUTPUT" | head -20 | sed 's/^/  /'
    add_failure "Test Coverage" "Coverage analysis failed" "Check pytest-cov installation and configuration. Debug output: $PWD/$COVERAGE_REPORT_FILE"
  fi
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# SECURITY AUDIT (BANDIT + SAFETY)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_SECURITY" == "true" ]]; then
  echo "ğŸ”’ Security Audit (bandit + safety)"

  SECURITY_PASSED=true

  # Run bandit for security issues in code (main source files only, with timeout)
  # Only fail on HIGH severity issues, ignore LOW/MEDIUM for now
  echo "ğŸ”§ Running bandit security scan..."
  BANDIT_OUTPUT=$(timeout 30s bandit -r . --exclude ./venv,./cursor-rules,./.venv,./logs,./tests -lll --format json 2>&1) || BANDIT_FAILED=true

  if [[ "$BANDIT_FAILED" == "true" ]]; then
    SECURITY_PASSED=false
    echo "âŒ Bandit security scan failed"

    # Try to extract meaningful security issues
    SECURITY_ISSUES=$(echo "$BANDIT_OUTPUT" | grep -E "(HIGH|MEDIUM)" | head -5)
    if [[ -n "$SECURITY_ISSUES" ]]; then
      echo "ğŸ“‹ Security Issues Found:"
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      echo "$SECURITY_ISSUES" | sed 's/^/  /'
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    fi
  else
    echo "   âœ… Bandit security scan passed"
  fi

  # Run safety scan for known vulnerabilities in dependencies
  echo "ğŸ”§ Running safety dependency scan..."
  echo "ğŸ“‹ Debug: Pre-execution checks:"
  echo "  Safety executable: $(which safety 2>&1 || echo 'NOT FOUND')"
  echo "  Safety version: $(safety --version 2>&1 || echo 'VERSION CHECK FAILED')"
  echo "  Python version: $(python --version 2>&1)"
  echo "  Working directory: $(pwd)"
  echo "  Requirements files present:"
  ls -la requirements*.txt 2>/dev/null | sed 's/^/    /' || echo "    No requirements files found"
  
  echo "ğŸ“‹ Debug: Testing basic safety command..."
  safety --help >/dev/null 2>&1 && echo "  Safety help works" || echo "  Safety help FAILED"
  
  echo "ğŸ“‹ Debug: Running safety scan with authentication..."
  set +e  # Don't exit on error
  # Use API key from environment if available, otherwise try without auth
  if [[ -n "$SAFETY_API_KEY" ]]; then
    echo "  Using Safety API key for authentication"
    SAFETY_OUTPUT=$(timeout 60s safety scan --output json --key "$SAFETY_API_KEY" 2>&1)
  else
    echo "  No Safety API key found, attempting scan without authentication"
    SAFETY_OUTPUT=$(timeout 60s safety scan --output json 2>&1)
  fi
  SAFETY_EXIT_CODE=$?
  
  # Also write to file to bypass GitHub truncation
  echo "$SAFETY_OUTPUT" > /tmp/safety_output.txt
  mkdir -p logs
  echo "$SAFETY_OUTPUT" > logs/safety_detailed_output.txt  # For artifact upload
  
  # Create comprehensive diagnostic file
  {
    echo "=== SAFETY SCAN DIAGNOSTIC REPORT ==="
    echo "Timestamp: $(date)"
    echo "Exit Code: $SAFETY_EXIT_CODE"
    echo "Output Length: ${#SAFETY_OUTPUT} characters"
    echo "Safety Version: $(safety --version 2>&1 || echo 'VERSION CHECK FAILED')"
    echo "Python Version: $(python --version 2>&1)"
    echo "Working Directory: $(pwd)"
    echo ""
    echo "=== COMPLETE SAFETY OUTPUT ==="
    echo "$SAFETY_OUTPUT"
    echo ""
    echo "=== HEXDUMP OF OUTPUT ==="
    echo "$SAFETY_OUTPUT" | hexdump -C
    echo ""
    echo "=== ALTERNATIVE COMMANDS ==="
    echo "--- Safety scan without JSON ---"
    timeout 10s safety scan 2>&1 || echo "Failed"
    echo ""
    echo "--- Safety check (deprecated) ---"
    timeout 10s safety check 2>&1 || echo "Failed"
  } > logs/safety_full_diagnostic.txt
  set -e  # Re-enable exit on error
  
  echo "ğŸ“‹ Debug: Safety command completed with exit code: $SAFETY_EXIT_CODE"
  echo "ğŸ“‹ Debug: Output length: ${#SAFETY_OUTPUT} characters"
  
  if [[ $SAFETY_EXIT_CODE -ne 0 ]]; then
    SAFETY_FAILED=true
  fi

  if [[ "$SAFETY_FAILED" == "true" ]]; then
    SECURITY_PASSED=false
    echo "âŒ Safety dependency check failed (exit code: $SAFETY_EXIT_CODE)"
    echo "ğŸ“‹ Full Safety Output for Debugging:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    # Show complete output for debugging (no truncation)
    if [[ ${#SAFETY_OUTPUT} -gt 0 ]]; then
      echo "ğŸ“‹ SAFETY OUTPUT (character by character to avoid truncation):"
      # Print each line with line numbers to force visibility
      echo "$SAFETY_OUTPUT" | nl -ba | sed 's/^/  /'
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      echo "ğŸ“‹ OUTPUT FROM FILE (to bypass GitHub truncation):"
      cat /tmp/safety_output.txt | sed 's/^/  /'
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      echo "ğŸ“‹ RAW OUTPUT (hexdump to see hidden characters):"
      echo "$SAFETY_OUTPUT" | hexdump -C | head -20 | sed 's/^/  /'
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      echo "ğŸ“‹ Output analysis:"
      echo "  Total characters: ${#SAFETY_OUTPUT}"
      echo "  Total lines: $(echo "$SAFETY_OUTPUT" | wc -l)"
      echo "  Contains 'vulnerabilities': $(echo "$SAFETY_OUTPUT" | grep -q "vulnerabilities" && echo "YES" || echo "NO")"
      echo "  Contains 'error': $(echo "$SAFETY_OUTPUT" | grep -qi "error" && echo "YES" || echo "NO")"
      echo "  Contains 'exception': $(echo "$SAFETY_OUTPUT" | grep -qi "exception" && echo "YES" || echo "NO")"
    else
      echo "  No output captured from safety command"
    fi
    
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ“‹ Debug: Trying safety scan without JSON output..."
    SAFETY_PLAIN_OUTPUT=$(timeout 10s safety scan 2>&1)
    if [[ $? -eq 0 ]]; then
      echo "  SUCCESS: Safety scan without JSON works!"
      echo "$SAFETY_PLAIN_OUTPUT" | sed 's/^/  /'
    else
      echo "  FAILED: Safety scan without JSON also failed"
      echo "$SAFETY_PLAIN_OUTPUT" | sed 's/^/  /'
    fi
    
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ“‹ Debug: Trying safety check (deprecated but might work)..."
    SAFETY_CHECK_OUTPUT=$(timeout 10s safety check 2>&1)
    if [[ $? -eq 0 ]]; then
      echo "  SUCCESS: Legacy safety check works!"
      echo "$SAFETY_CHECK_OUTPUT" | sed 's/^/  /'
    else
      echo "  FAILED: Legacy safety check also failed"
      echo "$SAFETY_CHECK_OUTPUT" | sed 's/^/  /'
    fi
    
    echo "ğŸ“‹ Debug: Environment information:"
    echo "  PWD: $(pwd)"
    echo "  PATH: $PATH" | head -c 200
    echo "  VIRTUAL_ENV: ${VIRTUAL_ENV:-'Not set'}"
    echo "  Python executable: $(which python)"
    echo "  Safety executable: $(which safety)"
    
    echo "ğŸ” DIAGNOSIS REQUIRED: Safety scan failed with no output"
    echo "ğŸ” Exit code $SAFETY_EXIT_CODE indicates:"
    case $SAFETY_EXIT_CODE in
      124) echo "   - Command timed out after 60 seconds" ;;
      127) echo "   - Command not found or not executable" ;;
      1) echo "   - General error or vulnerabilities found" ;;
      2) echo "   - Misuse of shell builtins" ;;
      *) echo "   - Unknown error condition" ;;
    esac
    
    # Extract vulnerability details from JSON output if possible
    if echo "$SAFETY_OUTPUT" | grep -q "vulnerabilities"; then
      # Try to extract package names and CVEs from JSON
      VULN_SUMMARY=$(echo "$SAFETY_OUTPUT" | grep -o '"package_name":"[^"]*"' | sed 's/"package_name":"//g' | sed 's/"//g' | head -5)
      if [[ -n "$VULN_SUMMARY" ]]; then
        echo "  Vulnerable packages found:"
        echo "$VULN_SUMMARY" | sed 's/^/    â€¢ /'
      fi
    fi
    
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
  else
    echo "   âœ… Safety dependency check passed"
  fi

  if [[ "$SECURITY_PASSED" == "true" ]]; then
    echo "âœ… Security Check: PASSED (bandit + safety)"
    add_success "Security Check" "No security vulnerabilities found"
  else
    echo "âŒ Security Check: FAILED (security issues found)"
    add_failure "Security Check" "Security vulnerabilities found" "See detailed output above and address security issues"
  fi
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# SONARQUBE QUALITY ANALYSIS
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_SONAR" == "true" ]]; then
  echo "ğŸ” SonarCloud Quality Analysis"

  SONAR_PASSED=true

  # Check if sonar-scanner is available
  if ! command -v sonar-scanner &> /dev/null; then
    echo "âŒ SonarCloud Scanner not found"
    echo "ğŸ“‹ Installation Instructions:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "  1. Download SonarScanner from: https://docs.sonarqube.org/latest/analysis/scan/sonarscanner/"
    echo "  2. Or install via Homebrew: brew install sonar-scanner"
    echo "  3. Configure SONAR_TOKEN environment variable for SonarCloud"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    add_failure "SonarCloud Analysis" "SonarCloud Scanner not installed" "Install sonar-scanner and configure environment variables"
    SONAR_PASSED=false
  else
    # Check if SONAR_TOKEN is set (SonarCloud doesn't need SONAR_HOST_URL)
    if [[ -z "$SONAR_TOKEN" ]]; then
      echo "âš ï¸  SonarCloud environment variables not configured"
      echo "ğŸ“‹ Required Environment Variables:"
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      echo "  export SONAR_TOKEN=your-sonarcloud-token"
      echo ""
      echo "  Get your token from: https://sonarcloud.io/account/security"
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      add_failure "SonarCloud Analysis" "Environment variables not configured" "Set SONAR_TOKEN environment variable"
      SONAR_PASSED=false
    else
      # Generate fresh coverage data and run SonarCloud analysis
      echo "ğŸ”§ Generating fresh coverage data for SonarCloud..."
      
      # Run tests with coverage to generate fresh coverage.xml and test-results.xml in root directory
      if python -m pytest tests/unit/ --cov=. --cov-report=xml:coverage.xml --cov-report=term-missing --junitxml=test-results.xml --tb=short -q; then
        echo "âœ… Coverage data generated successfully"
        
        # Run SonarCloud scanner with fresh data
        echo "ğŸ”§ Running SonarCloud analysis with fresh coverage data..."
        if sonar-scanner \
          -Dsonar.python.coverage.reportPaths=coverage.xml \
          -Dsonar.python.xunit.reportPath=test-results.xml \
          -Dsonar.qualitygate.wait=false; then
          echo "âœ… SonarCloud scanner completed successfully"
          
          # Now check the quality gate status
          echo "ğŸ”§ Checking SonarCloud quality gate status..."
          if python scripts/sonar_issues_scraper.py --project-key ScienceIsNeato_course_record_updater; then
            echo "âœ… SonarCloud Analysis: PASSED"
            add_success "SonarCloud Analysis" "All quality gate conditions met"
          else
            echo "âŒ SonarCloud Analysis: FAILED"
            echo "ğŸ“‹ See detailed issues above for specific fixes needed"
            add_failure "SonarCloud Analysis" "Quality gate failed with specific issues" "Fix the issues listed above and re-run analysis"
            SONAR_PASSED=false
          fi
        else
          echo "âŒ SonarCloud scanner failed"
          add_failure "SonarCloud Analysis" "SonarCloud scanner execution failed" "Check sonar-scanner configuration and network connectivity"
          SONAR_PASSED=false
        fi
      else
        echo "âŒ Failed to generate coverage data"
        add_failure "SonarCloud Analysis" "Coverage data generation failed" "Fix failing tests before running SonarCloud analysis"
        SONAR_PASSED=false
      fi
    fi
  fi

  if [[ "$SONAR_PASSED" != "true" ]]; then
    echo "ğŸ’¡ SonarQube provides comprehensive code quality analysis including:"
    echo "   â€¢ Code smells and maintainability issues"
    echo "   â€¢ Security vulnerabilities"
    echo "   â€¢ Code coverage analysis"
    echo "   â€¢ Technical debt assessment"
    echo "   â€¢ Duplication detection"
  fi
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# IMPORT ORGANIZATION CHECK
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_IMPORTS" == "true" ]]; then
  echo "ğŸ“¦ Import Organization Check"

  # Check import organization with isort
  IMPORT_OUTPUT=$(isort --check-only --diff --profile black --skip venv --skip .venv --skip-glob="**/venv/*" --skip-glob="**/.venv/*" . 2>&1) || IMPORT_FAILED=true

  if [[ "$IMPORT_FAILED" != "true" ]]; then
    echo "âœ… Import Organization: PASSED"
    add_success "Import Organization" "All imports properly organized"
  else
    echo "âŒ Import Organization: FAILED"
    echo "ğŸ“‹ Import Issues:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "$IMPORT_OUTPUT" | head -15 | sed 's/^/  /'
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    add_failure "Import Organization" "Import organization issues found" "Run 'isort --profile black *.py **/*.py' to fix"
  fi
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# CODE DUPLICATION CHECK
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_DUPLICATION" == "true" ]]; then
  echo "ğŸ”„ Code Duplication Check"

  # Check if npx is available for jscpd
  if ! command -v npx &> /dev/null; then
    echo "âš ï¸  npx not found, delegating to SonarCloud"
    echo "âœ… Duplication Check: DELEGATED TO SONARCLOUD"
    add_success "Duplication Check" "npx unavailable, duplication analysis delegated to SonarCloud"
  else
    echo "ğŸ”§ Running jscpd code duplication analysis..."
    
    # Run jscpd with appropriate settings for Python project
    # - min-lines 3: Detect duplications of 3+ lines (sensitive)
    # - threshold 5: Fail if >5% duplication (reasonable for Python)
    # - Focus on Python files with comprehensive exclusions
    DUPLICATION_OUTPUT=$(npx jscpd . \
      --min-lines 3 \
      --threshold 5 \
      --reporters console \
      --ignore '**/__tests__/**,**/*.test.*,**/tests/**,**/venv/**,**/.venv/**,**/__pycache__/**,**/*.pyc,**/node_modules/**,**/.git/**,**/logs/**,**/.scannerwork/**,**/python-database/**,**/codeql-**,**/cursor-rules/**' \
      --format python \
      --silent 2>&1) || DUPLICATION_FAILED=true

    if [[ "$DUPLICATION_FAILED" != "true" ]]; then
      # Extract duplication percentage from output
      DUPLICATION_PERCENT=$(echo "$DUPLICATION_OUTPUT" | grep -o '[0-9]\+\.[0-9]\+%' | tail -1 || echo "0.0%")
      CLONES_FOUND=$(echo "$DUPLICATION_OUTPUT" | grep -o '[0-9]\+ clones' | grep -o '[0-9]\+' || echo "0")
      
      echo "âœ… Code Duplication: PASSED ($DUPLICATION_PERCENT duplication, $CLONES_FOUND clones)"
      add_success "Code Duplication" "Duplication at $DUPLICATION_PERCENT with $CLONES_FOUND clones (below 5% threshold)"
    else
      echo "âŒ Code Duplication: FAILED"
      echo "ğŸ“‹ Duplication Analysis Results:"
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      echo "$DUPLICATION_OUTPUT" | head -20 | sed 's/^/  /'
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      
      # Extract duplication percentage for failure message
      DUPLICATION_PERCENT=$(echo "$DUPLICATION_OUTPUT" | grep -o '[0-9]\+\.[0-9]\+%' | tail -1 || echo "unknown")
      add_failure "Code Duplication" "Excessive duplication detected ($DUPLICATION_PERCENT)" "Review and refactor duplicated code blocks shown above"
    fi
  fi
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# SUMMARY REPORT
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

echo "ğŸ“Š Quality Gate Summary"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

if [[ ${#PASSED_CHECKS[@]} -gt 0 ]]; then
  echo "âœ… PASSED CHECKS (${#PASSED_CHECKS[@]}):"
  for check in "${PASSED_CHECKS[@]}"; do
    IFS='|' read -r name message <<< "$check"
    echo "   â€¢ $name: $message"
  done
  echo ""
fi

if [[ ${#FAILED_CHECKS_DETAILS[@]} -gt 0 ]]; then
  echo "âŒ FAILED CHECKS (${#FAILED_CHECKS_DETAILS[@]}):"
  for check in "${FAILED_CHECKS_DETAILS[@]}"; do
    IFS='|' read -r name reason suggestion <<< "$check"
    echo "   â€¢ $name: $reason"
    echo "     Fix: $suggestion"
  done
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸŸ¨ JAVASCRIPT LINTING CHECK (ESLint) ğŸŸ¨
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_JS_LINT" == "true" ]]; then
  echo "ğŸ” JavaScript Lint Check (ESLint)"

  # Check if Node.js and npm are available
  if ! command -v npm &> /dev/null; then
    echo "âš ï¸  npm not found, skipping JavaScript linting"
    echo "âœ… JavaScript Lint Check: SKIPPED (npm not available)"
    add_success "JavaScript Lint Check" "npm not available, JavaScript linting skipped"
  else
    # Check if node_modules exists, if not install dependencies
    if [ ! -d "node_modules" ]; then
      echo "ğŸ“¦ Installing JavaScript dependencies..."
      npm install --silent
    fi

    # Run ESLint on JavaScript files with auto-fix
    echo "ğŸ”§ Running ESLint analysis with auto-fix..."
    
    # First try to auto-fix
    if npm run lint:fix >/dev/null 2>&1; then
      echo "ğŸ”§ Auto-fixed JavaScript linting issues"
    fi
    
    # Then check if everything passes
    if npm run lint; then
      echo "âœ… JavaScript Lint Check: PASSED"
      add_success "JavaScript Lint Check" "All JavaScript files pass ESLint rules"
    else
      echo "âŒ JavaScript Lint Check: FAILED"
      add_failure "JavaScript Lint Check" \
                  "JavaScript files have linting errors that couldn't be auto-fixed" \
                  "Review ESLint output above and fix manually"
    fi
  fi
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ¨ JAVASCRIPT FORMATTING CHECK (Prettier) ğŸ¨
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_JS_FORMAT" == "true" ]]; then
  echo "ğŸ¨ JavaScript Format Check (Prettier)"

  # Check if Node.js and npm are available
  if ! command -v npm &> /dev/null; then
    echo "âš ï¸  npm not found, skipping JavaScript formatting"
    echo "âœ… JavaScript Format Check: SKIPPED (npm not available)"
    add_success "JavaScript Format Check" "npm not available, JavaScript formatting skipped"
  else
    # Check if node_modules exists, if not install dependencies
    if [ ! -d "node_modules" ]; then
      echo "ğŸ“¦ Installing JavaScript dependencies..."
      npm install --silent
    fi

    # Run Prettier to format and then check
    echo "ğŸ”§ Running Prettier auto-format and check..."
    
    # First auto-format
    if npm run format >/dev/null 2>&1; then
      echo "ğŸ”§ Auto-formatted JavaScript files"
    fi
    
    # Then verify formatting is correct
    if npm run format:check; then
      echo "âœ… JavaScript Format Check: PASSED"
      add_success "JavaScript Format Check" "All JavaScript files are properly formatted"
    else
      echo "âŒ JavaScript Format Check: FAILED"
      add_failure "JavaScript Format Check" \
                  "JavaScript files are not properly formatted after auto-fix" \
                  "Review Prettier output above and fix manually"
    fi
  fi
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ§ª JAVASCRIPT TEST SUITE (Jest) ğŸ§ª
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_JS_TESTS" == "true" ]]; then
  echo "ğŸ§ª JavaScript Test Suite (Jest)"

  # Check if Node.js and npm are available
  if ! command -v npm &> /dev/null; then
    echo "âš ï¸  npm not found, skipping JavaScript tests"
    echo "âœ… JavaScript Tests: SKIPPED (npm not available)"
    add_success "JavaScript Tests" "npm not available, JavaScript tests skipped"
  else
    # Check if node_modules exists, if not install dependencies
    if [ ! -d "node_modules" ]; then
      echo "ğŸ“¦ Installing JavaScript dependencies..."
      npm install --silent
    fi

    # Run Jest tests and capture detailed output
    echo "  ğŸ” Running JavaScript test suite..."
    JS_TEST_OUTPUT=$(npm run test:js 2>&1) || JS_TEST_FAILED=true
    
    if [[ "$JS_TEST_FAILED" == "true" ]]; then
      echo "âŒ JavaScript Tests: FAILED"
      echo ""
      echo "ğŸ“‹ Test Results:"
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      
      # Extract test summary from Jest output
      TEST_SUMMARY=$(echo "$JS_TEST_OUTPUT" | grep -E "Test Suites:|Tests:|Snapshots:|Time:" | sed 's/^/  /')
      if [[ -n "$TEST_SUMMARY" ]]; then
        echo "$TEST_SUMMARY"
      else
        echo "  Unable to parse test summary"
      fi
      
      # Extract failed test details
      FAILED_TESTS=$(echo "$JS_TEST_OUTPUT" | grep -A 5 "FAIL " | sed 's/^/  /')
      if [[ -n "$FAILED_TESTS" ]]; then
        echo ""
        echo "ğŸ“‹ Failed Tests:"
        echo "$FAILED_TESTS"
      fi
      
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      echo ""
      
      # Extract specific failure counts for better error message
      FAILED_COUNT=$(echo "$JS_TEST_OUTPUT" | grep -o '[0-9]* failed' | grep -o '[0-9]*' | head -1 || echo "unknown")
      TOTAL_COUNT=$(echo "$JS_TEST_OUTPUT" | grep -o '[0-9]* total' | grep -o '[0-9]*' | head -1 || echo "unknown")
      
      add_failure "JavaScript Tests" \
                  "$FAILED_COUNT of $TOTAL_COUNT tests failed" \
                  "Fix failing tests and run 'npm run test:js' for details"
    else
      echo "âœ… JavaScript Tests: PASSED"
      
      # Extract and display test summary for successful runs
      TEST_SUMMARY=$(echo "$JS_TEST_OUTPUT" | grep -E "Test Suites:|Tests:|Snapshots:|Time:" | sed 's/^/  /')
      if [[ -n "$TEST_SUMMARY" ]]; then
        echo ""
        echo "ğŸ“Š Test Summary:"
        echo "$TEST_SUMMARY"
      fi
      
      # Extract passed count for success message
      PASSED_COUNT=$(echo "$JS_TEST_OUTPUT" | grep -o '[0-9]* passed' | grep -o '[0-9]*' | head -1 || echo "all")
      TOTAL_COUNT=$(echo "$JS_TEST_OUTPUT" | grep -o '[0-9]* total' | grep -o '[0-9]*' | head -1 || echo "")
      
      if [[ -n "$TOTAL_COUNT" ]]; then
        add_success "JavaScript Tests" "$PASSED_COUNT of $TOTAL_COUNT tests passed"
      else
        add_success "JavaScript Tests" "All JavaScript tests passed"
      fi
    fi
  fi
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ“Š JAVASCRIPT COVERAGE ANALYSIS (80% threshold) ğŸ“Š
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_JS_COVERAGE" == "true" ]]; then
  echo "ğŸ“Š JavaScript Coverage Analysis (80% threshold)"

  # Check if Node.js and npm are available
  if ! command -v npm &> /dev/null; then
    echo "âš ï¸  npm not found, skipping JavaScript coverage"
    echo "âœ… JavaScript Coverage: SKIPPED (npm not available)"
    add_success "JavaScript Coverage" "npm not available, JavaScript coverage skipped"
  else
    # Check if node_modules exists, if not install dependencies
    if [ ! -d "node_modules" ]; then
      echo "ğŸ“¦ Installing JavaScript dependencies..."
      npm install --silent
    fi

            # Run Jest with coverage
            echo "  ğŸ” Running JavaScript coverage analysis..."
            JS_COVERAGE_OUTPUT=$(npm run test:coverage 2>&1) || JS_COVERAGE_FAILED=true
            
            if [[ "$JS_COVERAGE_FAILED" == "true" ]]; then
              echo "âŒ JavaScript Coverage: FAILED"
              echo ""
              echo "ğŸ“Š Coverage Results:"
              echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
              
              # Extract and display coverage summary table
              COVERAGE_TABLE=$(echo "$JS_COVERAGE_OUTPUT" | sed -n '/All files/,/^$/p' | head -10 | sed 's/^/  /')
              if [[ -n "$COVERAGE_TABLE" ]]; then
                echo "$COVERAGE_TABLE"
              fi
              
              # Extract individual coverage percentages
              STATEMENTS=$(echo "$JS_COVERAGE_OUTPUT" | grep -o 'Statements.*: [0-9.]*%' | grep -o '[0-9.]*%' | head -1 || echo "unknown")
              BRANCHES=$(echo "$JS_COVERAGE_OUTPUT" | grep -o 'Branches.*: [0-9.]*%' | grep -o '[0-9.]*%' | head -1 || echo "unknown")
              FUNCTIONS=$(echo "$JS_COVERAGE_OUTPUT" | grep -o 'Functions.*: [0-9.]*%' | grep -o '[0-9.]*%' | head -1 || echo "unknown")
              LINES=$(echo "$JS_COVERAGE_OUTPUT" | grep -o 'Lines.*: [0-9.]*%' | grep -o '[0-9.]*%' | head -1 || echo "unknown")
              
              echo ""
              echo "  ğŸ“ˆ Coverage Summary:"
              echo "    Statements: $STATEMENTS"
              echo "    Branches:   $BRANCHES"
              echo "    Functions:  $FUNCTIONS"
              echo "    Lines:      $LINES"
              echo "    Threshold:  80% (all categories)"
              
              echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
              echo ""
              
              # Extract line coverage percentage for comparison
              LINES_PERCENT=$(echo "$LINES" | grep -o '[0-9.]*' | head -1)
              LINES_INT=$(echo "$LINES_PERCENT" | cut -d. -f1)
              if [[ "$LINES_INT" -lt 80 ]]; then
                add_failure "JavaScript Coverage" \
                            "Line coverage threshold not met: $LINES (requires 80%)" \
                            "Add more tests to increase line coverage or run 'npm run test:coverage' for details"
              else
                echo "âœ… JavaScript Coverage: PASSED (Line coverage: $LINES)"
                add_success "JavaScript Coverage" "Line coverage threshold met: $LINES (80% required)"
              fi
            else
              # Extract and display coverage summary for successful runs
              STATEMENTS=$(echo "$JS_COVERAGE_OUTPUT" | grep -o 'Statements.*: [0-9.]*%' | grep -o '[0-9.]*%' | head -1 || echo "80%+")
              BRANCHES=$(echo "$JS_COVERAGE_OUTPUT" | grep -o 'Branches.*: [0-9.]*%' | grep -o '[0-9.]*%' | head -1 || echo "80%+")
              FUNCTIONS=$(echo "$JS_COVERAGE_OUTPUT" | grep -o 'Functions.*: [0-9.]*%' | grep -o '[0-9.]*%' | head -1 || echo "80%+")
              LINES=$(echo "$JS_COVERAGE_OUTPUT" | grep -o 'Lines.*: [0-9.]*%' | grep -o '[0-9.]*%' | head -1 || echo "80%+")
              
              echo "âœ… JavaScript Coverage: PASSED"
              echo ""
              echo "ğŸ“Š Coverage Summary:"
              echo "  Statements: $STATEMENTS"
              echo "  Branches:   $BRANCHES" 
              echo "  Functions:  $FUNCTIONS"
              echo "  Lines:      $LINES âœ… (threshold: 80%)"
              
              add_success "JavaScript Coverage" "Line coverage threshold met: $LINES (80% required)"
            fi
  fi
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# SMOKE TESTS EXECUTION - END-TO-END TESTING
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_SMOKE_TESTS" == "true" ]]; then
  echo "ğŸ”¥ Smoke Tests Execution (End-to-End Validation)"
  
  # Colors for output
  RED='\033[0;31m'
  GREEN='\033[0;32m'
  YELLOW='\033[1;33m'
  BLUE='\033[0;34m'
  NC='\033[0m' # No Color
  
  # Test configuration
  DEFAULT_PORT=${DEFAULT_PORT:-3001}
  TEST_URL="http://localhost:$DEFAULT_PORT"
  SERVER_PID=""
  
  # Function to check if Chrome/Chromium is available
  check_chrome() {
    # Check for Chrome in common locations (works with modern Selenium)
    if command -v google-chrome >/dev/null 2>&1; then
      echo -e "${GREEN}âœ… Chrome found in PATH${NC}"
      return 0
    elif command -v chromium-browser >/dev/null 2>&1; then
      echo -e "${GREEN}âœ… Chromium found in PATH${NC}"
      return 0
    elif command -v chromium >/dev/null 2>&1; then
      echo -e "${GREEN}âœ… Chromium found in PATH${NC}"
      return 0
    elif [ -f "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome" ]; then
      echo -e "${GREEN}âœ… Chrome found in Applications (macOS)${NC}"
      return 0
    elif [ -f "/Applications/Chromium.app/Contents/MacOS/Chromium" ]; then
      echo -e "${GREEN}âœ… Chromium found in Applications (macOS)${NC}"
      return 0
    elif [ -f "/usr/bin/google-chrome-stable" ]; then
      echo -e "${GREEN}âœ… Chrome found in /usr/bin (CI/Linux)${NC}"
      return 0
    elif [ -f "/usr/bin/google-chrome" ]; then
      echo -e "${GREEN}âœ… Chrome found in /usr/bin (CI/Linux)${NC}"
      return 0
    else
      echo -e "${RED}âŒ Chrome/Chromium not found. Please install Chrome or Chromium for frontend tests${NC}"
      echo -e "${YELLOW}ğŸ’¡ On macOS: brew install --cask google-chrome${NC}"
      echo -e "${YELLOW}ğŸ’¡ On Ubuntu/CI: sudo apt-get install google-chrome-stable${NC}"
      return 1
    fi
  }
  
  # Function to start test server
  start_test_server() {
    echo -e "${BLUE}ğŸš€ Starting test server on port $DEFAULT_PORT...${NC}"
    
    # Load environment variables
    if [ -f ".envrc" ]; then
      source .envrc
    fi
    
    # Activate virtual environment if it exists
    if [ -d "venv" ]; then
      source venv/bin/activate
    fi
    
    # Start server on test port in background
    PORT=$DEFAULT_PORT python app.py > logs/test_server.log 2>&1 &
    SERVER_PID=$!
    
    # Wait for server to start
    echo -e "${BLUE}â³ Waiting for server to start...${NC}"
    for i in {1..30}; do
      if curl -s "$TEST_URL" > /dev/null 2>&1; then
        echo -e "${GREEN}âœ… Test server started successfully${NC}"
        return 0
      fi
      sleep 1
    done
    
    echo -e "${RED}âŒ Test server failed to start${NC}"
    kill $SERVER_PID 2>/dev/null || true
    return 1
  }
  
  # Function to stop test server
  stop_test_server() {
    if [ ! -z "$SERVER_PID" ]; then
      echo -e "${BLUE}ğŸ›‘ Stopping test server...${NC}"
      kill $SERVER_PID 2>/dev/null || true
      
      # Wait for process to terminate
      for i in {1..10}; do
        if ! kill -0 $SERVER_PID 2>/dev/null; then
          break
        fi
        sleep 1
      done
      
      # Force kill if still running
      kill -9 $SERVER_PID 2>/dev/null || true
      echo -e "${GREEN}âœ… Test server stopped${NC}"
    fi
  }
  
  # Function to run smoke tests
  run_smoke_tests() {
    echo -e "${BLUE}ğŸ§ª Running smoke tests...${NC}"
    
    # Install test dependencies if needed
    pip install -q pytest selenium requests 2>/dev/null || true
    
    # Test Selenium WebDriver setup
    python -c "
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
try:
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)
    print('âœ… Selenium WebDriver setup verified')
    driver.quit()
except Exception as e:
    print(f'âŒ Selenium WebDriver setup failed: {e}')
    exit(1)
" || {
      echo -e "${RED}âŒ Selenium WebDriver setup failed${NC}"
      return 1
    }
    
    # Run the smoke tests
    pytest tests/smoke/ -v --tb=short
    TEST_EXIT_CODE=$?
    
    if [ $TEST_EXIT_CODE -eq 0 ]; then
      echo -e "${GREEN}âœ… All smoke tests passed!${NC}"
    else
      echo -e "${RED}âŒ Some smoke tests failed${NC}"
    fi
    
    return $TEST_EXIT_CODE
  }
  
  # Main smoke test execution
  echo "  ğŸ” Checking prerequisites..."
  
  # Create logs directory
  mkdir -p logs
  
  # Check Chrome availability
  if ! check_chrome; then
    add_failure "Smoke Tests" "Chrome/Chromium not available" "Install Chrome or Chromium for frontend tests"
    echo ""
    return
  fi
  
  # SQLite is used for persistence; no external emulator required.

  # Start test server
  if ! start_test_server; then
    add_failure "Smoke Tests" "Test server failed to start" "Check server logs and ensure port $DEFAULT_PORT is available"
    echo ""
    return
  fi
  
  # Run tests
  run_smoke_tests
  TEST_RESULT=$?
  
  # Stop test server
  stop_test_server
  
  # Report results
  if [ $TEST_RESULT -eq 0 ]; then
    echo -e "${GREEN}ğŸ‰ All smoke tests completed successfully!${NC}"
    echo -e "${GREEN}ğŸ“Š The application UI is working correctly${NC}"
    add_success "Smoke Tests" "All smoke tests passed successfully"
  else
    echo -e "${RED}ğŸ’¥ Smoke tests failed!${NC}"
    echo -e "${RED}ğŸ” Check test output above for details${NC}"
    echo -e "${YELLOW}ğŸ’¡ Common issues:${NC}"
    echo -e "${YELLOW}   - JavaScript errors in browser console${NC}"
    echo -e "${YELLOW}   - Missing HTML elements${NC}"
    echo -e "${YELLOW}   - API endpoints not responding${NC}"
    echo -e "${YELLOW}   - Static assets not loading${NC}"
    add_failure "Smoke Tests" "Smoke test failures detected" "See detailed output above and run 'pytest tests/smoke/ -v' for full details"
  fi
  echo ""
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# FRONTEND CHECK - QUICK UI VALIDATION
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
if [[ "$RUN_FRONTEND_CHECK" == "true" ]]; then
  echo "ğŸŒ Frontend Check (Quick UI Validation)"
  
  # Run the frontend check script
  echo "  ğŸ” Running frontend validation check..."
  FRONTEND_OUTPUT=$(./check_frontend.sh 2>&1) || FRONTEND_FAILED=true
  
  if [[ "$FRONTEND_FAILED" == "true" ]]; then
    echo "âŒ Frontend Check: FAILED"
    echo ""
    echo "ğŸ“‹ Frontend Check Output:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "$FRONTEND_OUTPUT" | sed 's/^/  /'
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    
    add_failure "Frontend Check" "Frontend validation failed" "See detailed output above and run './check_frontend.sh' manually"
  else
    echo "âœ… Frontend Check: PASSED"
    add_success "Frontend Check" "Frontend validation passed successfully"
  fi
  echo ""
fi

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

if [[ $FAILED_CHECKS -eq 0 ]]; then
  echo "ğŸ‰ ALL CHECKS PASSED!"
  echo "âœ… Ready to commit with confidence!"
  echo ""
  echo "ğŸš€ Course Record Updater quality validation completed successfully!"
  exit 0
else
  echo "âŒ QUALITY GATE FAILED"
  echo "ğŸ”§ $FAILED_CHECKS check(s) need attention"
  echo ""
  echo "ğŸ’¡ Fix the issues above and run the checks again"
  exit 1
fi
